{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab6a7c5-9904-49da-9d69-bc30e4b92d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\anaconda3\\envs\\myenv\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from collections import deque \n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57a949f-26cb-4660-8765-c12f6705a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062e1e57-8930-4fea-a784-28ea2eaf10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.Input(shape=(input_size,)),\n",
    "                tf.keras.layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                tf.keras.layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"policy\")\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        logits = self.model(observations)\n",
    "        return tfp.distributions.Categorical(logits=logits)\n",
    "\n",
    "    def sampel_action(self, observations):\n",
    "        sampled_actions = self.action_distribution(observations).sample().numpy()\n",
    "        return sampled_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9863180a-9948-4c07-ae08-4f6e7e5471a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaselineNet():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.model = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.Input(shape=(input_size,)),\n",
    "                tf.keras.layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
    "                tf.keras.layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
    "            ],\n",
    "            name=\"baseline\")\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        output = tf.squeeze(self.model(observations))\n",
    "        return output\n",
    "\n",
    "    def update(self, observations, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.forward(observations)\n",
    "            loss = tf.keras.losses.mean_squared_error(y_true=target, y_pred=predictions)\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa35fe1-a95c-4aee-b48b-d52aa6c89759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_plot(ys, ylabel, title, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(ys)), ys)\n",
    "    plt.xlabel(\"Training Episode\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33c2519-cc90-4d32-a4f5-4f57be691215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient(object):\n",
    "    def __init__(self, env, num_iterations=300, batch_size=2000, max_ep_len=500, output_path=\"results/\"):\n",
    "        self.output_path = output_path\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        self.env = env\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "        self.policy_net = PolicyNet(input_size=self.observation_dim, output_size=self.action_dim)\n",
    "        self.baseline_net = BaselineNet(input_size=self.observation_dim, output_size=1)\n",
    "        \n",
    "    def play_games(self, env=None, num_episodes = None):\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "        if not env:\n",
    "            env = self.env\n",
    "\n",
    "        while (num_episodes or t < self.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "                        \n",
    "            for step in range(self.max_ep_len):\n",
    "                env.render()\n",
    "                states.append(state)\n",
    "                action = self.policy_net.sampel_action(np.atleast_2d(state))[0]\n",
    "                \n",
    "                state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.batch_size:\n",
    "                    break\n",
    "\n",
    "            path = {\"observation\": np.array(states),\n",
    "                    \"reward\": np.array(rewards),\n",
    "                    \"action\": np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "        return paths, episode_rewards\n",
    "        \n",
    "    def get_returns(self, paths):\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            reversed_rewards = np.flip(rewards,0)\n",
    "            g_t = 0\n",
    "            for r in reversed_rewards:\n",
    "                g_t = r + self.gamma*g_t\n",
    "                returns.insert(0, g_t)\n",
    "            all_returns.append(returns)\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "\n",
    "    def get_advantage(self, returns, observations):\n",
    "        values = self.baseline_net.forward(observations).numpy()\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages-np.mean(advantages)) / np.sqrt(np.sum(advantages**2))\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        observations = tf.convert_to_tensor(observations)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        advantages = tf.convert_to_tensor(advantages)\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_prob = self.policy_net.action_distribution(observations).log_prob(actions)\n",
    "            loss = -tf.math.reduce_mean(log_prob * tf.cast(advantages, tf.float32))\n",
    "        grads = tape.gradient(loss, self.policy_net.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.model.trainable_weights))\n",
    "\n",
    "    def train(self):\n",
    "        all_total_rewards = []\n",
    "        averaged_total_rewards = []\n",
    "        for t in range(self.num_iterations):\n",
    "            paths, total_rewards = self.play_games()\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.get_advantage(returns, observations)\n",
    "            self.baseline_net.update(observations=observations, target=returns)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            print(\"Average reward for batch {}: {:04.2f}\".format(t,avg_reward))\n",
    "        print(\"Training complete\")\n",
    "        np.save(self.output_path+ \"rewards.npy\", averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Reward\", \"CartPole-v1\", self.output_path + \"rewards.png\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73a1fe1-d6b3-4528-bbdb-21b038cfe464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Average reward for batch 0: 21.97\n",
      "Average reward for batch 1: 35.84\n",
      "Average reward for batch 2: 45.14\n",
      "Average reward for batch 3: 58.53\n",
      "Average reward for batch 4: 88.95\n",
      "Average reward for batch 5: 98.00\n",
      "Average reward for batch 6: 118.88\n",
      "Average reward for batch 7: 177.09\n",
      "Average reward for batch 8: 162.17\n",
      "Average reward for batch 9: 188.20\n",
      "Average reward for batch 10: 192.20\n",
      "Average reward for batch 11: 260.71\n",
      "Average reward for batch 12: 306.83\n",
      "Average reward for batch 13: 383.75\n",
      "Average reward for batch 14: 231.88\n",
      "Average reward for batch 15: 197.10\n",
      "Average reward for batch 16: 155.50\n",
      "Average reward for batch 17: 117.50\n",
      "Average reward for batch 18: 125.47\n",
      "Average reward for batch 19: 158.83\n",
      "Average reward for batch 20: 189.30\n",
      "Average reward for batch 21: 230.38\n",
      "Average reward for batch 22: 248.62\n",
      "Average reward for batch 23: 222.00\n",
      "Average reward for batch 24: 212.33\n",
      "Average reward for batch 25: 282.00\n",
      "Average reward for batch 26: 500.00\n",
      "Average reward for batch 27: 500.00\n",
      "Average reward for batch 28: 500.00\n",
      "Average reward for batch 29: 500.00\n",
      "Average reward for batch 30: 500.00\n",
      "Average reward for batch 31: 500.00\n",
      "Average reward for batch 32: 500.00\n",
      "Average reward for batch 33: 500.00\n",
      "Average reward for batch 34: 500.00\n",
      "Average reward for batch 35: 500.00\n",
      "Average reward for batch 36: 500.00\n",
      "Average reward for batch 37: 500.00\n",
      "Average reward for batch 38: 500.00\n",
      "Average reward for batch 39: 500.00\n",
      "Average reward for batch 40: 500.00\n",
      "Average reward for batch 41: 500.00\n",
      "Average reward for batch 42: 280.14\n",
      "Average reward for batch 43: 163.83\n",
      "Average reward for batch 44: 98.70\n",
      "Average reward for batch 45: 57.12\n",
      "Average reward for batch 46: 64.63\n",
      "Average reward for batch 47: 89.32\n",
      "Average reward for batch 48: 99.84\n",
      "Average reward for batch 49: 97.70\n",
      "Average reward for batch 50: 103.68\n",
      "Average reward for batch 51: 82.08\n",
      "Average reward for batch 52: 98.95\n",
      "Average reward for batch 53: 110.56\n",
      "Average reward for batch 54: 124.94\n",
      "Average reward for batch 55: 126.13\n",
      "Average reward for batch 56: 129.93\n",
      "Average reward for batch 57: 131.67\n",
      "Average reward for batch 58: 144.00\n",
      "Average reward for batch 59: 142.62\n",
      "Average reward for batch 60: 125.33\n",
      "Average reward for batch 61: 133.33\n",
      "Average reward for batch 62: 128.53\n",
      "Average reward for batch 63: 147.92\n",
      "Average reward for batch 64: 162.25\n",
      "Average reward for batch 65: 190.50\n",
      "Average reward for batch 66: 190.00\n",
      "Average reward for batch 67: 189.00\n",
      "Average reward for batch 68: 208.33\n",
      "Average reward for batch 69: 215.78\n",
      "Average reward for batch 70: 232.88\n",
      "Average reward for batch 71: 196.89\n",
      "Average reward for batch 72: 210.11\n",
      "Average reward for batch 73: 197.50\n",
      "Average reward for batch 74: 205.67\n",
      "Average reward for batch 75: 190.50\n",
      "Average reward for batch 76: 206.11\n",
      "Average reward for batch 77: 229.25\n",
      "Average reward for batch 78: 296.67\n",
      "Average reward for batch 79: 393.75\n",
      "Average reward for batch 80: 482.50\n",
      "Average reward for batch 81: 500.00\n",
      "Average reward for batch 82: 500.00\n",
      "Average reward for batch 83: 500.00\n",
      "Average reward for batch 84: 500.00\n",
      "Average reward for batch 85: 500.00\n",
      "Average reward for batch 86: 500.00\n",
      "Average reward for batch 87: 500.00\n",
      "Average reward for batch 88: 500.00\n",
      "Average reward for batch 89: 500.00\n",
      "Average reward for batch 90: 500.00\n",
      "Average reward for batch 91: 500.00\n",
      "Average reward for batch 92: 500.00\n",
      "Average reward for batch 93: 500.00\n",
      "Average reward for batch 94: 500.00\n",
      "Average reward for batch 95: 500.00\n",
      "Average reward for batch 96: 500.00\n",
      "Average reward for batch 97: 500.00\n",
      "Average reward for batch 98: 500.00\n",
      "Average reward for batch 99: 500.00\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "model = PolicyGradient(env, num_iterations=100)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80f9b64-d624-43d8-9b3c-2dbe3ff9d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988a7d26-8b92-4c63-ba13-f4c6a429e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: REINFORCE_cartpole_policy\\assets\n"
     ]
    }
   ],
   "source": [
    "model.policy_net.model.save(\"REINFORCE_cartpole_policy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
